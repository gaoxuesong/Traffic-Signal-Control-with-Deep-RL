{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-28 16:36:31,870] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-25a4396f7572>:54 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-12-28 16:36:32,265] From <ipython-input-7-25a4396f7572>:54 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terminated\n",
      "-100\n",
      "reward:-0.482434\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tflearn\n",
    "import random\n",
    "import pdb\n",
    "from AC_networks import ActorNet, CriticNet\n",
    "import gym\n",
    "game = gym.make('LunarLander-v2')\n",
    "state_dim = game.observation_space.shape[0]\n",
    "action_dim = game.action_space.n\n",
    "# info = graph, dim_s, dim_a, hid_layers, learning_rate, tau\n",
    "MAX_EPISODE = 10000\n",
    "MAX_STEP = 1000\n",
    "MINIBATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "class input_struct:\n",
    "    def __init__(self, net):\n",
    "        self.net = net\n",
    "\n",
    "info = input_struct('actor')\n",
    "try:\n",
    "    actor_t\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    actor_t.reset_net()\n",
    "try:\n",
    "    critic_t\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    critic_t.reset_net()  \n",
    "    \n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    info.graph = graph\n",
    "    info.dim_s = state_dim\n",
    "    info.dim_a = action_dim\n",
    "    info.act_hid_layers = np.array([16])\n",
    "    info.crit_hid_layers = np.array([16])\n",
    "    info.learning_rate = LEARNING_RATE\n",
    "    info.tau = 0.1\n",
    "    info.ent_beta = 0.01\n",
    "    actor_t = ActorNet(info)\n",
    "    critic_t = CriticNet(info, actor_t.get_num_trainable_vars())\n",
    "\n",
    "EP = 29000 - 1\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    actor_t.set_up(sess)\n",
    "    critic_t.set_up(sess)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver = tf.train.Saver()\n",
    "    filename =  './saved_nets/Episod_' + str(EP) + '.chk'\n",
    "    saver.restore(sess, filename)\n",
    "    \n",
    "    ep_reward = 0\n",
    "    s = game.reset()\n",
    "    for st in xrange(MAX_STEP):\n",
    "        game.render()\n",
    "        #print(actor.give_policy(s.reshape(-1,info.dim_s)))\n",
    "        A = actor_t.take_action(s.reshape(-1,info.dim_s))\n",
    "        a = np.argmax(A)\n",
    "        s2, r, terminal, report = game.step(a)\n",
    "        #print(r)\n",
    "        s = s2\n",
    "        ep_reward += (1.*r)\n",
    "        if terminal:\n",
    "            #print(r)\n",
    "            print('terminated')\n",
    "            break\n",
    "    print(r)\n",
    "    print('reward:%f' %np.mean(ep_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(.95**100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(1e-3*.99**100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from AC_networks import ActorNet, CriticNet\n",
    "dir(CriticNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v2 = tf.Variable([4])\n",
    "v1 = tf.Variable([1])\n",
    "v1 = v2\n",
    "a = tf.Print(v1, [v1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f3206cc1cd0>> ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
