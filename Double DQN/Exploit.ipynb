{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from collections import deque\n",
    "import random\n",
    "import pdb\n",
    "from replay_memory_Q import ReplayMemory\n",
    "from doubleq_networks import DoubleQ_Network\n",
    "from test_game import test_env\n",
    "from info_obj import info_holder\n",
    "import os\n",
    "import gym\n",
    "\n",
    "#game_name = 'LunarLander-v2'\n",
    "#game_name = 'CartPole-v0'\n",
    "game_name = 'MountainCar-v0'\n",
    "game = gym.make(game_name)\n",
    "state_dim = game.observation_space.shape[0]\n",
    "action_dim = game.action_space.n\n",
    "save_dir = './Results/'+ game_name[:-3] + '/net/'\n",
    "string = ['hidden layers: ']\n",
    "hid_lay = 8\n",
    "with open(save_dir + 'params.txt', 'r') as f:\n",
    "    xa = f.readline()\n",
    "    while xa:\n",
    "        if string[0] in xa:\n",
    "            hid_lay = [int(xb) for xb in xa[len(string[0]):].split()]\n",
    "        xa = f.readline()\n",
    "print(hid_lay)\n",
    "MAX_STEP = 1000\n",
    "LEARNING_RATE = 0\n",
    "TAU = 0\n",
    "ENTROPY_BETA = 0\n",
    "eps = 0\n",
    "info = info_holder()\n",
    "try:\n",
    "    DQN\n",
    "except:\n",
    "    pass\n",
    "else:\n",
    "    DQN.reset_net()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    info.graph = graph\n",
    "    info.dim_s = state_dim\n",
    "    info.dim_a = action_dim\n",
    "    info.hid_layers = np.array(hid_lay)\n",
    "    info.learning_rate = LEARNING_RATE\n",
    "    info.tau = TAU\n",
    "    info.entropy_beta = ENTROPY_BETA\n",
    "    DQN = DoubleQ_Network(info)\n",
    "\n",
    "EP = 13000 - 1\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "    DQN.set_up(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    filename = save_dir + 'Episod_' + str(EP) + '.chk'\n",
    "    saver.restore(sess, filename)\n",
    "    ep_reward = np.zeros(100)\n",
    "    for i in range(100):\n",
    "        s = game.reset()\n",
    "        a = DQN.take_action(s.reshape(-1,info.dim_s), eps)\n",
    "        s2, r, terminal, report = game.step(a)\n",
    "        \n",
    "        for st in range(MAX_STEP):\n",
    "            #game.render()\n",
    "            a = DQN.take_action(s.reshape(-1,info.dim_s), eps)\n",
    "            s2, r, terminal, report = game.step(a)\n",
    "            s = s2\n",
    "            ep_reward[i] += (1.*r)\n",
    "            if terminal:\n",
    "                #print('terminated')\n",
    "                break\n",
    "        #print(ep_reward[i])\n",
    "        #pdb.set_trace()\n",
    "    print('reward:%f' %np.mean(ep_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
