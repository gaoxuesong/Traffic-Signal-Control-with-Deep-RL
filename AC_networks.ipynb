{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tflearn\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorNet(object):\n",
    "    \n",
    "    def __init__(self, info):\n",
    "        self.graph = info.graph\n",
    "        self.dim_s = info.dim_s\n",
    "        self.dim_a = info.dim_a\n",
    "        self.hid_layers = info.act_hid_layers\n",
    "        #self.learning_rate = info.learning_rate\n",
    "        self.tau = info.tau\n",
    "        self.learning_rate = tf.placeholder(tf.float32)\n",
    "        \n",
    "        #tf.reset_default_graph()\n",
    "        # Actor Network\n",
    "        self.inputs, self.policy = self.create_net()\n",
    "        self.network_params = tf.trainable_variables()\n",
    "        self.no_params = len(tf.trainable_variables())\n",
    "\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_policy = self.create_net()\n",
    "        #self.target_network_params = tf.trainable_variables()[len(self.network_params)/2:]\n",
    "        self.target_no_params = len(tf.trainable_variables()) - self.no_params\n",
    "\n",
    "        # Op for periodically updating target network with online network weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.network_params[self.no_params + i].assign(tf.mul(self.network_params[i], self.tau) + \\\n",
    "                tf.mul(self.network_params[self.no_params + i], 1. - self.tau))\n",
    "                for i in range(self.target_no_params)]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.td = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # taken action (input for policy)\n",
    "        self.a = tf.placeholder(tf.float32, [None, self.dim_a])\n",
    "        #######################################################\n",
    "        ######## Probably has to be changed ###################\n",
    "        ######## With log of pi (look at https://github.com/miyosuda/async_deep_reinforce/blob/master/game_ac_network.py\n",
    "        #######################################################\n",
    "        # Combine the gradients here \n",
    "        self.responsible_outputs = tf.reduce_sum(self.policy * self.a, [1])\n",
    "        self.log_pi = tf.log(self.responsible_outputs)\n",
    "        \n",
    "        self.actor_loss = - tf.reduce_sum( self.log_pi * self.td)\n",
    "        self.actor_gardients = tf.Variable(tf.zeros_like(self.network_params))\n",
    "        self.reset_gradients = tf.assign(self.actor_gradients, tf.zeros_like(self.actor_gradients))\n",
    "        self.add_gradients = tf.add(self.actor_gradients, tf.gradients(self.actor_loss, self.network_params, self.td))\n",
    "        \n",
    "        #self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.actor_loss)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "                        apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = self.no_params + self.target_no_params\n",
    "        \n",
    "    def create_net(self):\n",
    "        \n",
    "        inputs = tflearn.input_data(shape=[None, self.dim_s])\n",
    "        net = tflearn.fully_connected(inputs, self.hid_layers[0], activation='relu')\n",
    "        \n",
    "        if len(self.hid_layers) > 1:\n",
    "            for h in self.hid_layers[1:]:\n",
    "                net = tflearn.fully_connected(net, h, activation='relu')\n",
    "        \n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        policy = tflearn.fully_connected(net, self.dim_a, activation='softmax', weights_init=w_init)\n",
    "        #pdb.set_trace()\n",
    "        return inputs, policy\n",
    "\n",
    "    def set_up(self, sess):\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, inputs, actions, td): #, lr_rate):\n",
    "        \n",
    "        #taken_action = self.take_action(inputs)\n",
    "        #pdb.set_trace()\n",
    "        self.sess.run(self.add_gradients, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.a: actions,\n",
    "            self.td: td,\n",
    "            #self.learning_rate: lr_rate\n",
    "            #self.action_gradient: a_gradient\n",
    "        })\n",
    "        \n",
    "    def apply_grads(self, lr_rate):\n",
    "        \n",
    "        #taken_action = self.take_action(inputs)\n",
    "        #pdb.set_trace()\n",
    "        self.sess.run(self.reset_gradients)\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            #self.inputs: inputs,\n",
    "            #self.a: actions,\n",
    "            #self.td: td,\n",
    "            self.learning_rate: lr_rate\n",
    "            #self.action_gradient: a_gradient\n",
    "        })\n",
    "        \n",
    "    def give_policy(self, inputs):\n",
    "        \n",
    "        return self.sess.run(self.policy, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "    \n",
    "    def take_action(self, inputs):\n",
    "        \n",
    "        pi = self.give_policy(inputs)\n",
    "        pi = pi/np.sum(pi, axis = 1).reshape(-1,1)\n",
    "        taken_action = np.zeros_like(pi)\n",
    "        #pdb.set_trace()\n",
    "        for i in xrange(pi.shape[0]):\n",
    "            taken_action[i, np.random.choice(self.dim_a, 1, p=pi[i,:].reshape(self.dim_a))] = 1\n",
    "        return taken_action\n",
    "    \n",
    "    def target_take_action(self):\n",
    "        pi = self.sess.run(self.target_policy, feed_dict={\n",
    "            self.inputs: inputs})\n",
    "        taken_action = np.zeros(self.dim_a)\n",
    "        return taken_action[np.random.choice(self.dim_a, 1, p=np.reshape(pi, self.dim_a))]\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "        \n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "    \n",
    "    def reset_net(self):\n",
    "        tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CriticNet(object):\n",
    "    \n",
    "    def __init__(self, info, num_actor_vars):\n",
    "        self.graph = info.graph\n",
    "        self.dim_s = info.dim_s\n",
    "        self.dim_a = info.dim_a\n",
    "        self.hid_layers = info.crit_hid_layers\n",
    "        #self.learning_rate = info.learning_rate\n",
    "        self.tau = info.tau\n",
    "        tf.placeholder(tf.float32, [None, self.dim_a])\n",
    "        \n",
    "        # Actor Network\n",
    "        self.inputs, self.value  = self.create_net()\n",
    "\n",
    "        self.no_params = len(tf.trainable_variables()) - num_actor_vars\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_value = self.create_net()\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "        #self.target_network_params = tf.trainable_variables()[(len(self.network_params)/2 + num_actor_vars):]\n",
    "        self.target_no_params = len(tf.trainable_variables()) - self.no_params - num_actor_vars\n",
    "\n",
    "        #pdb.set_trace()\n",
    "        # Op for periodically updating target network with online network weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.network_params[self.no_params + i].assign(tf.mul(self.network_params[i], self.tau) + \\\n",
    "                tf.mul(self.network_params[self.no_params + i], 1. - self.tau))\n",
    "                for i in range(self.target_no_params)]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        #self.td = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "        # taken action (input for policy)\n",
    "        #self.a = tf.placeholder(tf.float32, [None, self.dim_a])\n",
    "\n",
    "        # Network target (y_i) r+ gamma*V_target(s2)\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.critic_loss = tflearn.mean_square(self.R, self.value)\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.critic_loss)\n",
    "        \n",
    "    def create_net(self):\n",
    "        \n",
    "        inputs = tflearn.input_data(shape=[None, self.dim_s])\n",
    "        # works with V instead of Q, uncomment action for Q\n",
    "        # action = tflearn.input_data(shape=[None, self.dim_a])\n",
    "        net = tflearn.fully_connected(inputs, self.hid_layers[0], activation='relu')\n",
    "        \n",
    "        #net = tflearn.fully_connected(net, self.hid_layers[1])\n",
    "        #t2 = tflearn.fully_connected(action, self.hid_layers[1])\n",
    "        #net = tflearn.activation(tf.matmul(net,t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        for h in self.hid_layers[1:]:\n",
    "            net = tflearn.fully_connected(net, h, activation='relu')\n",
    "        \n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        \n",
    "        return inputs, out #, action\n",
    "    \n",
    "    def set_up(self, sess):\n",
    "        pdb.set_trace()\n",
    "        self.sess = sess\n",
    "        \n",
    "    def train(self, inputs, R, lr_rate):\n",
    "        pdb.set_trace()\n",
    "        return self.sess.run([self.value, self.optimize], feed_dict={\n",
    "                self.inputs: inputs,\n",
    "                self.R: R,\n",
    "                self.learning_rate: lr_rate\n",
    "            })\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.value, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "    \n",
    "    def predict_target(self, inputs):\n",
    "\n",
    "        return self.sess.run(self.target_value, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "        \n",
    "    def reset_net(self):\n",
    "        tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
